* endisukaj.dev
:PROPERTIES:
:ONE: one-default-home-list-pages
:CUSTOM_ID: /
:END:

* Software abstraction barrier
:PROPERTIES:
:ONE: one-default
:CUSTOM_ID: /blog/software-abstraction-barrier/
:END:

If we think about software as built in layers, we can imagine it as a pyramid
where the lower levels are the closest to the hardware and the higher you go the
closer you get to the user. Each level adds some abstraction to the one below it
by hiding any implementation detail that is not useful to that particular level.
This way, we can write code at a higher level while freeing our limited mental
capacity by letting go of the need to know every implementation detail.

Thinking about abstraction also helps when working between teams as it helps
with delegating responsibilities between teams.

For example, consider a scenario in which Team A is working on building a data
pipeline for a machine learning project. Team B needs to access specific data in
particular points as it's passing through the data pipeline (e.g. for
statistical analysis reasons). In this case we would have two possibilities:

1. Team A writes specific code every time Team B asks for something specific
2. Team A writes an abstraction over their existing code, e.g. a public function
   or library which Team B can directly use to write their specific tasks as
   needed.

By writing an abstraction over their existing code (read: layer in the software
pyramid), Team A made it possible for Team B to access what they need without
caring to learn how the underlying software layer is implemented. In other
words, Team A implemented an abstraction barrier which is a layer of functions
that hide the implementation so well that you can completely forget about how it
is implemented even when using those functions. It should be noted that the "not
caring" is symmetrical and works both ways. Team B does not care about how the
data pipeline is implemented, but Team A also does not care about how Team B
implement their solutions using the abstraction barrier.

** When to use abstraction barriers:

*** 1. To facilitate changes of implementation

In cases of high uncertainty about how you want to implement something, an abstraction barrier can be the layer of indirection that lets you change the implementation later. This property might be useful if you are prototyping something and you still don't know how best to implement it. Or perhaps you _know_ something will change; you're just not ready to do it yet, like if you know you will want to get data from the server eventually, but for now you'll just stub it out.

*** 2. To make code easier to write and read
Abstraction barriers allow us to ignore details. Sometimes those details are bug magnets. Did we initialize the loop variables correctly? Did we have an off-by-one error in the loop exit condition? An abstraction barrier that lets you ignore those details will make your code easier to write. If you hide the right details, then less adept programmers can be productive when using it.

*** 3. To reduce coordination between teams
Our development team could change things without talking with marketing first. And marketing could write simple marketing campaigns without checking in with development. The abstraction barrier allows teams on either side to ignore the details the other team handles. Thus, each team moves faster.

*** 4. To mentally focus on the problem at hand
They let you think more easily about the problem you are trying to solve. An abstraction barrier makes some details unimportant to the problem we are solving right now. It means we are less likely to make a mistake and less likely to get tired.

* Dataclass vs TypedDict in Python
:PROPERTIES:
:ONE: one-default
:CUSTOM_ID: /blog/dataclass-vs-typeddict-in-python/
:END:

Type annotations were introduced to Python with [[https://peps.python.org/pep-0484/][PEP484]] and since then they have
been a staple tool for most Python programs. They open up Python code to easier
static analysis and refactoring and potential runtime type checking. Not to
mention making the code a lot easier to reason with in general.

Take the following example:

#+begin_src python
	def hello(a):
		return "hello " + a

	hello(2)
#+end_src


The above code would rase this error:

#+begin_src python
	TypeError: can only concatenate str (not "int") to str
#+end_src


However, using type annotations would help us catch this error before the code is even executed:

#+begin_src python
	def hello(a: str) -> str:
		return "hello " + a

	hello(2)
#+end_src

Trying to compile this code with mypy would yield the following error:

#+begin_src python
	error: Argument 1 to "hello" has incompatible type "int"
#+end_src

Of course, using a good IDE or text editor with good static analysis tools would
help us catch this error while writing the code.

Obviously, Python type annotations work with classes as well. For example:

#+begin_src python
	import requests
	from typing import List

	class Book:
		title: str
		authors: List[str]

	def get_book_from_api(endpoint: str) -> Book:
		response = requests.get(endpoint)
		response_json = response.json()
		book = Book()
		book.title = response_json["title"]
		book.authors = response_json["authors"]

		return book
#+end_src

In the example above, the `Book` class works as a wrapper around some data we
get from a rest endpoint. It contains two fields, a title field and an authors
field. Using Python type annotations we have defined the types of both those
fields. However, assigning the values to each field one by one is a bit
annoying, and the OOP way to do that would be to assign attributes inside the
=__init__= method:

#+begin_src python
	class Book:
		title: str
		authors: List[str]

		def __init__(self, title: str, authors: List[str]):
			self.title = title
			self.authors = authors

	def get_book_from_api(endpoint: str) -> Book:
		response = requests.get(endpoint)
		response_json = response.json()
		book = Book(title=response_json["title"], authors=response_json["authors"])

		return book
#+end_src


This is great, we're making important steps here. We managed to create a Python
class that uses type annotations and can be constructed using the =__init__=
method. But this class is still just a wrapper around some data, so it would be
great if we can reduce the boilerplate for this code while still keeping all the
advantages that the Python type system gives us

This is where =dataclass= becomes useful. It's a Python decorator that
automatically adds the =__init__= method (including types!) and some additional
goodies to a Python class. It can be used as follows:

#+begin_src python
	import requests
	from dataclasses import dataclass
	from typing import List

	@dataclass
	class Book:
		title: str
		authors: List[str]

	def get_book_from_api(endpoint: str) -> Book:
		response = requests.get(endpoint)
		response_json = response.json()
		book = Book(title=response_json["title"], authors=response_json["authors"])

		return book
#+end_src


As you we can see, there's no need to manually define the =__init__= method
anymore. Not only does this reduce the time needed to write classes since it
reduces the amount of boilerplate code, it also reduces the probability of
errors since there's less boilerplate code for us to type in.

However, dataclasses are not a silver bullet and have some drawbacks. But in
order to understand those drawbacks we must first introduce the notion of [[https://en.wikipedia.org/wiki/Duck_typing][duck
typing]]. The idea behind it is simple: if it quacks like a duck and walks like
a duck then it's a duck. In other words, an object is of a particular type if it
has all the methods and attributes required by that type.

Duck typing cannot be followed with dataclasses though. Consider the following
example:

#+begin_src python
	@dataclass
	class Table:
		color: str
		age: int

	@dataclass
	class Bookshelf:
		color: str
		age: int

	Table(color="brown", age=12) == Bookshelf(color="brown", age=12) # False
#+end_src

Following the logic of duck typing, then both the =Table= and the =Bookshelf=
objects should represent the same type since they have the same attributes and
the same values. However, they are not treated as equal objects because they
belong to different classes. In order to strip that difference between both
objects we have to resort to dictionaries:

#+begin_src python
	{"color": "brown", "age": 12} == {"color": "brown", "age": 12} # True
#+end_src

But using Python dicts would make us lose all the advantages that Python types
gives us, right? Wrong. This is where =TypedDict= comes into play. =TypedDict=
basically allows Python dicts to work as types:

#+begin_src python
	from typing import TypedDict

	class Table(TypedDict):
	    color: str
	    age: int

	class Bookshelf(TypedDict):
	    color: str
	    age: int

	Table(color="brown", age=12) == Bookshelf(color="brown", age=12) # True

#+end_src

However, even though =TypedDict= can give us all the advantages of duck typing,
it has two big disadvantages compared to dataclasses:

1. Validation is harder
2. Not immutable

Let's start with the first point. Since dataclasses can have explicit =__init__=
methods, then they can be validated whenever an object of that type is
constructed. This is not true for =TypedDict= and every time we construct a
dictionary of a specific type, then we'd have to validate all its values at
runtime.

Whereas for immutability, in dataclasses its ensured by the =frozen=True=
argument. If set, it ensures that any object that belongs to that type would be
immutable, which is not possible with =TypedDict=.
* The Feynman technique
:PROPERTIES:
:ONE: one-default
:CUSTOM_ID: /blog/feynman-technique/
:END:

Named after the phycisist Richard Feynman, it is designed to help individuals
learn and understand subjects deeply. It aligns well with the practice of
writing about what you learn, and its principles can be summarized as follows:

- Choose a concept: Select a concept or topic you want to understand
- Teach it to a child: Try to explain the concept as if you were teaching it to
  a child. Use simple language and avoid jargon.
- Identify gaps and go back to the source material: If you find areas where your
  explanation falters, go back to the source material to fill in the gaps.
- Simplify and use analogies: Create analogies and use simple language to make
  the concept more relatable and easier to grasp.
- Review and refine: Continuously review and refine your explanation until you
  can articulate the concept in the simplest terms possible.
* Haskell libraries
:PROPERTIES:
:ONE: one-default
:CUSTOM_ID: /blog/haskell-libraries/
:END:

One of the biggest downsides of the Haskell ecosystem IMO is the lack of good
libraries. Or rather, well-documented libraries and I am not talking about
docstrings here. Most library authors will write some comments and let the
document generator and type system do the work for them. It's not how it works
and Haskell will never take off until this issue has been tackled. Most people
will have a hard time figuring out how to make a real-world project, while
people who are not beginners will keep hitting brick wall after brick wall until
they either reach some sort of God-like understanding of Haskell or give up
entirely and rewrite the project in another language.

This is not entirely on the library authors either. For some reason the entire
Haskell community is adverse to writing good documentation and/or tutorials.
Everyone seems to be enamored with the idea that the terminology used should be
as cryptic and unaccessible as possible and that everyone should be able to
completely grasp the type system and all the various intricacies that one finds
in a language like Haskell since day one. Now before someone starts thinking
about the reasons why this is, I'd like to mention that I know very well about
Haskell's start in academia and how deeply rooted in maths it is, but
unfortunately most software engineers and most importantly project managers do
not care about function purity or how you can rewrite crucial parts of an
infrastructure by using co-monoids and functors. What matters are results, ease
of development, and a strong knowledge base that unfortunately is lacking.
* Useful latency numbers
:PROPERTIES:
:ONE: one-default
:CUSTOM_ID: /blog/useful-latency-numbers/
:END:

1ns = 10^(-9)s

1μs = 10^(-6)s

1ms = 10^(-3)s


1ns - 10ns ---> L1/L2 Cache

10ns - 100ns ---> L3 cache

100ns - 1μs ---> [System call] -> [C library] -> Kernel or MD5 hashing 64 bit number

1μs - 10μs ---> Context switching between threads. Depends on the size of data

10μs - 100μs ---> process HTTP request or sequential of a 8KB file from SSD

100μs 1ms ---> SSD write latency or intra-zone networking round-trip or Memcache/Redis GET

1ms - 10ms ---> Intra-zone networking latency or seektime of HDD.

10ms - 100ms ---> Network round-trip for US-EU or read 1GB from main memory

100ms - 1s ---> bcrypt a password or TLS handshake or network roundtrip between US west coast and Singapore or 1GB sequential read from SSD.

1s+ ---> Transfer 1GB over the network within the same region.
* Using Python schema to validate scraped objects
:PROPERTIES:
:ONE: one-default
:CUSTOM_ID: /blog/using-python-schema-to-validate-scraped-objects/
:END:

Web scraping describes the various techniques employed in extracting data from
websites. More often than not, this is done programmatically using tiny programs
called spiders. However, web scraping offers insidious challenges and obstacles
that might not be immediately obvious. One of them is issues with **data
validity.**

Usually the final objective of web scraping is extracting structured data from
semi-structured or unstructured content. The structured data often comes with a
prerequisite of data validity. In other words, for the structured data that we
need to scrape, certain (or all) data points needs to satisfy some requirements
in order to be considered valid. For example, let’s assume that we have to
scrape property data from a collection of websites that list new properties for
sale. We need to scrape the following data (among others):

- price (in USD)
- surface
- number of rooms
- location

For all of them we need to make sure that they are found in each object in the
output, and that they are of the correct type and value. For example, *price,
number of rooms, and surface* all need to be numbers. *location* must be a
string. Furthermore, *number of rooms* must not be too big (there’s not that
many houses with over 100 rooms on the market, if any). If any of these
conditions are not met, then it means that there is a problem with scraping and
parsing one (or more) of the pages and we risk sending faulty data downstream.

Most large-scale web crawling solutions will have some sort of pipeline in their
workflow that is dedicated to validating and processing the scraped data. One
way to do validation is by using ad-hoc logic that makes sure that the data
points are of the type and value required. Often this involves complex
multi-step checks and algorithms and it’s hard to scale and update as new data
types are scraped or if old ones have updated requirements.

To avoid this, I like using a Python library called [[https://github.com/keleshev/schema][schema]]. It allows you to
validate JSON or YAML data using Python data types and logic. To continue the
example above, for the property data we scrape, we can define the following
schema:

#+begin_src python
    from schema import Schema, And, Use, Optional, SchemaError

    schema = Schema([{
    	'price': int,
    	'surface':  int,
    	'number_of_rooms': And(int, lambda n: 0 < n and n <= 100),
        'location': str
    }])
#+end_src


Finally in the pipeline we can simply do:

#+begin_src python
    schema.validate(data)
#+end_src


If the data is valid then it will return the data, otherwise a =SchemaError=
exception will be thrown.

Doing this we can add a clear validation layer that is separate from any
processing layer which is easy to read, update, and scale as needed.
* Emacs static site generators
:PROPERTIES:
:ONE: one-default
:CUSTOM_ID: /blog/emacs-static-site-generators/
:END:

Since I came back to Emacs, I've been using org-mode daily to organize my TODOs,
agenda, and notes. In fact, org-mode was one of the reasons I came back to
Emacs. As my writing is deeply coupled with my research process which is done in
part using org-roam, it would make sense, in my opinion, to connect the writing
and publishing process to it and be able to do everything inside Emacs, in true
emacsian fashion.

Preferrably the writing would be done in org-mode and the process would be a
simple and automated one that would take me from an org-mode file to a blog
post. Ideally, the entire blog would be a series of interconnected org files.
** One.el
https://github.com/tonyaldon/one.el
*** Pros
- Can be generated with a single Emacs command
- Can be modified with Elisp and CSS
- HTML templates are plain Elisp data
- No config file
- No external dependencies
*** Cons
- Everything is contained in one single org file
*** Installation
Installation is pretty easy, you can install it from MELPA with =package-install=
*** Usage
Generating a blank project is pretty easy as well, you can run
=one-default-new-project= inside a folder and it will generate a new project for
you with some sane defaults.

After you generate the project, you can simply run =one-build= to generate the
site under a folder called =./public/=.

** Org Publish
https://orgmode.org/manual/Publishing.html
*** Pros
- Built into org-mode
- No additional dependencies
- Highly configurable
- Can be configured to export to various formats, not just HTML (e.g. PDF,
  Markdown, etc)
*** Cons
- Needs some work to get up and running
